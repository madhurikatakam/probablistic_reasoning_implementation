{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3a2dc5-9f71-42a0-9f07-cb44ec0d43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from scipy.stats import chi2_contingency\n",
    "from itertools import permutations, combinations, product``666666666666666666`\n",
    "import math, random, time\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1bdc3-1b3b-4d32-9ed0-6159a87e3e7b",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "We load the stroke dataset and view basic details.  \n",
    "This gives us an idea of what data we will work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf220e-8eff-48c4-8fd9-4e46b8f55d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:\\\\Semester 5\\\\Probabilistic Reasoning\\\\Project\\\\archive\\\\healthcare-dataset-stroke-data.csv\")\n",
    "print(\"Rows,Cols:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd95f4ed-7bda-4f99-8f71-6c81408bb32a",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "We handle missing values and fix any inconsistent entries.  \n",
    "This prepares the data for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d22677-b31c-4f55-8f42-a59ca2f27891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                  0\n",
      "hypertension         0\n",
      "heart_disease        0\n",
      "avg_glucose_level    0\n",
      "bmi                  0\n",
      "smoking_status       0\n",
      "stroke               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols_needed = ['age','hypertension','heart_disease','avg_glucose_level','bmi','smoking_status','stroke']\n",
    "# if dataset has more features you may include them; keep this minimal for speed\n",
    "for c in cols_needed:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Column {c} not found, available columns: {df.columns.tolist()}\")\n",
    "df = df[cols_needed].copy()\n",
    "\n",
    "# Impute BMI median\n",
    "df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n",
    "\n",
    "# Simplify smoking_status (merge 'Unknown' with 'never smoked' if exists)\n",
    "if 'smoking_status' in df.columns:\n",
    "    df['smoking_status'] = df['smoking_status'].replace('Unknown', 'never smoked')\n",
    "\n",
    "# Convert stroke to string labels\n",
    "df['stroke'] = df['stroke'].map({0:'no', 1:'yes'})\n",
    "\n",
    "# Quick check\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f1b9eb-7c90-4077-a4cb-07ecb159eea2",
   "metadata": {},
   "source": [
    "### Discretize Continuous Columns\n",
    "We convert continuous values into categories (bins).\n",
    "Bayesian Networks work better with discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f470064-1438-4ef7-a23d-bb7de3c1a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_cols = ['age','avg_glucose_level','bmi']\n",
    "disc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "df[disc_cols] = disc.fit_transform(df[disc_cols]).astype(int).astype(str)\n",
    "\n",
    "# Make sure all columns are strings (categorical)\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "018fe3c5-5ee5-4407-9345-30ff0511d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "stroke_yes = df[df['stroke'] == 'yes']\n",
    "stroke_no = df[df['stroke'] == 'no']\n",
    "\n",
    "stroke_yes_up = resample(stroke_yes, \n",
    "                         replace=True,\n",
    "                         n_samples=len(stroke_no),\n",
    "                         random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([stroke_yes_up, stroke_no]).sample(frac=1, random_state=42)\n",
    "df = df_balanced.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee93df5-042c-46a6-bedf-107c22f0a9d4",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "We split data into training and testing parts.  \n",
    "Train data builds the model and test data checks accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eacf3cc-19bf-4365-93a5-dfd21a32d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test sizes: (7291, 7) (2431, 7)\n",
      "Variables used: ['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df['stroke'])\n",
    "print(\"Train/test sizes:\", train_df.shape, test_df.shape)\n",
    "\n",
    "# List of variables\n",
    "vars_all = list(train_df.columns)\n",
    "print(\"Variables used:\", vars_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375a68db-763f-4544-9611-8a98d1b48f1e",
   "metadata": {},
   "source": [
    "### Handle Class Imbalance\n",
    "We oversample stroke cases to balance the dataset.  \n",
    "This helps the model learn stroke patterns properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c307a18-0c7f-4505-9ec4-d3da75fe13e8",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "These functions support structure learning and scoring.  \n",
    "Used inside the learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6b0aa8-489c-4fee-b386-b3be15854a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_indep(x, y):\n",
    "    table = pd.crosstab(x, y)\n",
    "    if table.size == 0 or table.shape[0] < 2 or table.shape[1] < 2:\n",
    "        return 1.0\n",
    "    chi2, p, _, _ = chi2_contingency(table)\n",
    "    return p\n",
    "\n",
    "def ci_test(df, X, Y, Z=None, alpha=0.05):\n",
    "    if not Z:\n",
    "        return chi2_indep(df[X], df[Y]) >= alpha\n",
    "    total_chi2 = 0.0\n",
    "    total_dof = 0\n",
    "    for _, sub in df.groupby(Z):\n",
    "        if sub.shape[0] < 5:\n",
    "            continue\n",
    "        table = pd.crosstab(sub[X], sub[Y])\n",
    "        if table.size == 0 or table.shape[0] < 2 or table.shape[1] < 2:\n",
    "            continue\n",
    "        chi2, p, dof, _ = chi2_contingency(table)\n",
    "        total_chi2 += chi2\n",
    "        total_dof += dof\n",
    "    if total_dof == 0:\n",
    "        return False\n",
    "    p_comb = 1 - math.exp(math.log(1 - 0.0) * 1)  # placeholder - rather compute via chi2 cdf\n",
    "    # more straightforward:\n",
    "    from scipy.stats import chi2 as chi2dist\n",
    "    p_comb = 1 - chi2dist.cdf(total_chi2, total_dof)\n",
    "    return p_comb >= alpha\n",
    "\n",
    "def has_cycle(edges, nodes):\n",
    "    adj = {n:[] for n in nodes}\n",
    "    for u,v in edges:\n",
    "        adj[u].append(v)\n",
    "    visited = {n:0 for n in nodes}\n",
    "    def dfs(u):\n",
    "        if visited[u]==1: return True\n",
    "        if visited[u]==2: return False\n",
    "        visited[u]=1\n",
    "        for w in adj[u]:\n",
    "            if dfs(w): return True\n",
    "        visited[u]=2\n",
    "        return False\n",
    "    for n in nodes:\n",
    "        if visited[n]==0 and dfs(n): return True\n",
    "    return False\n",
    "\n",
    "def compute_cpt_counts(df, child, parents):\n",
    "    res = {}\n",
    "    if not parents:\n",
    "        c = df[child].value_counts()\n",
    "        res[()] = c.to_dict()\n",
    "    else:\n",
    "        for pvals, group in df.groupby(parents):\n",
    "            key = pvals if isinstance(pvals, tuple) else (pvals,)\n",
    "            res[key] = group[child].value_counts().to_dict()\n",
    "    return res\n",
    "\n",
    "def bic_score(df, edges, nodes):\n",
    "    N = len(df)\n",
    "    parents = {n:[] for n in nodes}\n",
    "    for u,v in edges:\n",
    "        parents[v].append(u)\n",
    "    LL = 0.0\n",
    "    num_params = 0\n",
    "    for child in nodes:\n",
    "        child_states = sorted(df[child].unique())\n",
    "        par = parents[child]\n",
    "        if not par:\n",
    "            counts = df[child].value_counts().to_dict()\n",
    "            total = sum(counts.values())\n",
    "            for s in child_states:\n",
    "                c = counts.get(s, 0)\n",
    "                if c>0:\n",
    "                    p = c/total\n",
    "                    LL += c * math.log(p)\n",
    "            num_params += (len(child_states)-1)\n",
    "        else:\n",
    "            grouped = df.groupby(par)\n",
    "            for _, subset in grouped:\n",
    "                total = len(subset)\n",
    "                if total == 0: continue\n",
    "                counts = subset[child].value_counts().to_dict()\n",
    "                for s in child_states:\n",
    "                    c = counts.get(s, 0)\n",
    "                    if c>0:\n",
    "                        p = c/total\n",
    "                        LL += c * math.log(p)\n",
    "                num_params += (len(child_states)-1)\n",
    "    bic = LL - 0.5 * num_params * math.log(N)\n",
    "    return bic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fe1527f-f0ab-4552-8ba9-8d2befe0c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_edges = {\n",
    "    ('age','stroke'),\n",
    "    ('hypertension','stroke'),\n",
    "    ('heart_disease','stroke'),\n",
    "    ('avg_glucose_level','stroke')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322e7518-e0bb-45f4-8d0e-be78262d71a2",
   "metadata": {},
   "source": [
    "### Learn BN Structure\n",
    "We use hill-climbing to find the best network shape.  \n",
    "The model improves step by step using BIC score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2340ae90-9420-42e6-896e-78169b75a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climb_structure(df, nodes, init_edges=set(), max_iter=200):\n",
    "    edges = set(init_edges)\n",
    "    best = bic_score(df, edges, nodes)\n",
    "    it = 0\n",
    "    while it < max_iter:\n",
    "        it += 1\n",
    "        improved = False\n",
    "        best_move = None\n",
    "        best_score = best\n",
    "        # consider add\n",
    "        for u,v in permutations(nodes,2):\n",
    "            if (u,v) in edges: continue\n",
    "            cand = set(edges); cand.add((u,v))\n",
    "            if has_cycle(cand, nodes): continue\n",
    "            score = bic_score(df, cand, nodes)\n",
    "            if score > best_score:\n",
    "                best_score = score; best_move = ('add',(u,v))\n",
    "        # consider remove\n",
    "        for (u,v) in list(edges):\n",
    "            cand = set(edges); cand.remove((u,v))\n",
    "            score = bic_score(df, cand, nodes)\n",
    "            if score > best_score:\n",
    "                best_score = score; best_move = ('remove',(u,v))\n",
    "        # reverse\n",
    "        for (u,v) in list(edges):\n",
    "            cand = set(edges); cand.remove((u,v)); cand.add((v,u))\n",
    "            if has_cycle(cand, nodes): continue\n",
    "            score = bic_score(df, cand, nodes)\n",
    "            if score > best_score:\n",
    "                best_score = score; best_move = ('reverse',(u,v))\n",
    "        if best_move:\n",
    "            typ,edge = best_move\n",
    "            if typ=='add': edges.add(edge)\n",
    "            elif typ=='remove': edges.remove(edge)\n",
    "            else:\n",
    "                u,v = edge; edges.remove((u,v)); edges.add((v,u))\n",
    "            best = best_score\n",
    "            improved = True\n",
    "        if not improved:\n",
    "            break\n",
    "    return edges, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c0817bd-028b-4a44-a777-9b2ad42b2a91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m edges, score \u001b[38;5;241m=\u001b[39m hill_climb_structure(sample, nodes, init_edges\u001b[38;5;241m=\u001b[39minit_edges, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "edges, score = hill_climb_structure(sample, nodes, init_edges=init_edges, max_iter=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ae785-0230-4974-b57e-48143a9a2c82",
   "metadata": {},
   "source": [
    "### Learn Probabilities\n",
    "We calculate CPTs using training data.  \n",
    "These probabilities help in predicting stroke risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2732b-5d82-479a-a955-70f6783b7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_learn_cpts(df, edges, nodes, laplace=1e-6):\n",
    "    parents = {n:[] for n in nodes}\n",
    "    for u,v in edges:\n",
    "        parents[v].append(u)\n",
    "    cpts = {}\n",
    "    for child in nodes:\n",
    "        par = parents[child]\n",
    "        counts = compute_cpt_counts(df, child, par)\n",
    "        child_vals = sorted(df[child].unique())\n",
    "        cpt = {}\n",
    "        for pc in counts:\n",
    "            total = sum(counts[pc].values())\n",
    "            probs = {}\n",
    "            for val in child_vals:\n",
    "                probs[val] = (counts[pc].get(val,0)+laplace) / (total + laplace*len(child_vals))\n",
    "            cpt[pc] = probs\n",
    "        cpts[child] = {'parents': par, 'cpt': cpt}\n",
    "    return cpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbc2472f-3e8f-40cf-b073-887cb78400bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob_from_cpts(cpts, var_domains, evidence, target='stroke'):\n",
    "    # returns probability P(target='yes' | evidence)\n",
    "    # build full enumeration over parents of target if necessary\n",
    "    info = cpts[target]\n",
    "    parents = info['parents']\n",
    "    child_vals = var_domains[target]\n",
    "    target_yes = 'yes'\n",
    "    # find parent values we have (if any parent not in evidence, enumerate)\n",
    "    missing_parents = [p for p in parents if p not in evidence]\n",
    "    probs_sum = 0.0\n",
    "    total_weight = 0.0\n",
    "    # enumerate over all possible assignments to missing parents\n",
    "    if missing_parents:\n",
    "        vals_list = [var_domains[p] for p in missing_parents]\n",
    "        for combo in product(*vals_list):\n",
    "            # build parent tuple in same order as stored\n",
    "            full_par_vals = []\n",
    "            i=0\n",
    "            for p in parents:\n",
    "                if p in evidence:\n",
    "                    full_par_vals.append(evidence[p])\n",
    "                else:\n",
    "                    full_par_vals.append(combo[i]); i+=1\n",
    "            key = tuple(full_par_vals) if len(full_par_vals)>1 else (full_par_vals[0],)\n",
    "            probs = info['cpt'].get(key, None)\n",
    "            if not probs:\n",
    "                # fallback to uniform\n",
    "                prob_yes = 1.0/len(child_vals)\n",
    "            else:\n",
    "                prob_yes = probs.get(target_yes, 0.0)\n",
    "            probs_sum += prob_yes\n",
    "            total_weight += 1.0\n",
    "        return probs_sum / total_weight if total_weight>0 else 0.0\n",
    "    else:\n",
    "        key = tuple(evidence[p] for p in parents) if parents else ()\n",
    "        probs = info['cpt'].get(key, None)\n",
    "        if not probs:\n",
    "            return 0.0\n",
    "        return probs.get(target_yes, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8db34-f9c6-42f0-9531-f07915d4da32",
   "metadata": {},
   "source": [
    "### Train Ensemble Models\n",
    "We train multiple BNs using bootstrap samples.  \n",
    "This reduces errors and improves stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29bebcf3-b7ad-413e-a46b-93cbb9660c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model 1/5  (BIC -37442.74)\n",
      "Trained model 2/5  (BIC -37221.41)\n",
      "Trained model 3/5  (BIC -37178.23)\n",
      "Trained model 4/5  (BIC -37171.08)\n",
      "Trained model 5/5  (BIC -37102.07)\n",
      "Ensemble training time: 200.11134815216064\n"
     ]
    }
   ],
   "source": [
    "def train_ensemble(train_df, nodes, M=7):\n",
    "    ensemble = []\n",
    "    bic_scores = []\n",
    "    start = time.time()\n",
    "    for m in range(M):\n",
    "        sample = train_df.sample(frac=1.0, replace=True, random_state=42+m)\n",
    "        # hill-climb structure (start empty), limit iterations for speed\n",
    "        edges, score = hill_climb_structure(sample, nodes, max_iter=80)\n",
    "        cpts = mle_learn_cpts(sample, edges, nodes)\n",
    "        ensemble.append({'edges': edges, 'cpts': cpts, 'bic': score})\n",
    "        bic_scores.append(score)\n",
    "        print(f\"Trained model {m+1}/{M}  (BIC {score:.2f})\")\n",
    "    print(\"Ensemble training time:\", time.time()-start)\n",
    "    return ensemble\n",
    "\n",
    "# choose small nodes to speed up (use the columns we have)\n",
    "nodes = vars_all  # e.g., ['age','hypertension','heart_disease','avg_glucose_level','bmi','smoking_status','stroke']\n",
    "\n",
    "# For speed, you may remove less informative nodes; but we'll try with these 7\n",
    "ensemble = train_ensemble(train_df, nodes, M=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1d460-07eb-41fc-9745-1d2675d142b6",
   "metadata": {},
   "source": [
    "### Predict Stroke\n",
    "We combine predictions from all BN models.  \n",
    "This gives better results than a single model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d714b54-6e65-4b5f-9e72-e1b0c5e1474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_domains = {c: sorted(train_df[c].unique()) for c in train_df.columns}\n",
    "\n",
    "# ===========================\n",
    "# 11. Predict on test set: average and BIC-weighted average\n",
    "# ===========================\n",
    "y_true = []\n",
    "y_pred_avg = []\n",
    "y_pred_bma = []\n",
    "prob_list_avg = []\n",
    "prob_list_bma = []\n",
    "\n",
    "# compute weights from BIC (convert to positive weights; higher bic -> higher weight)\n",
    "bics = np.array([m['bic'] for m in ensemble])\n",
    "# to avoid numerical issues, shift\n",
    "w = np.exp(bics - bics.max())\n",
    "weights = w / w.sum()\n",
    "\n",
    "for idx,row in test_df.iterrows():\n",
    "    ev = {c: row[c] for c in row.index if c!='stroke'}\n",
    "    y_true.append(row['stroke'])\n",
    "    probs = []\n",
    "    for model in ensemble:\n",
    "        prob = predict_prob_from_cpts(model['cpts'], var_domains, ev, target='stroke')\n",
    "        probs.append(prob)\n",
    "    # average\n",
    "    p_avg = np.mean(probs)\n",
    "    p_bma = np.dot(weights, probs)\n",
    "    prob_list_avg.append(p_avg)\n",
    "    prob_list_bma.append(p_bma)\n",
    "    y_pred_avg.append('yes' if p_avg>0.5 else 'no')\n",
    "    y_pred_bma.append('yes' if p_bma>0.5 else 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50ed76-c789-441d-8391-e12b173c0cd1",
   "metadata": {},
   "source": [
    "### Evaluate Performance\n",
    "We measure how well the model predicts stroke.  \n",
    "Accuracy, precision, recall, and AUC are reported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f5a4468-d02c-4d72-b784-5ae8c8bad43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Simple Average Ensemble ====\n",
      "Accuracy: 0.7786918963389552\n",
      "Precision: 0.7352328005559416\n",
      "Recall: 0.8707818930041152\n",
      "F1: 0.7972871137905049\n",
      "AUC: 0.8445222817847087\n",
      "Confusion matrix:\n",
      " [[ 835  381]\n",
      " [ 157 1058]]\n",
      "\n",
      "==== BIC-weighted Ensemble (approx BMA) ====\n",
      "Accuracy: 0.7803373097490744\n",
      "Precision: 0.7336993822923816\n",
      "Recall: 0.8798353909465021\n",
      "F1: 0.8001497005988024\n",
      "AUC: 0.8222844920944337\n",
      "Confusion matrix:\n",
      " [[ 828  388]\n",
      " [ 146 1069]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_true, y_pred, probs=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, pos_label='yes')\n",
    "    rec = recall_score(y_true, y_pred, pos_label='yes')\n",
    "    f1 = f1_score(y_true, y_pred, pos_label='yes')\n",
    "    auc = roc_auc_score([1 if y=='yes' else 0 for y in y_true], probs) if probs is not None else None\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:\", rec)\n",
    "    print(\"F1:\", f1)\n",
    "    if auc is not None: print(\"AUC:\", auc)\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred, labels=['no','yes']))\n",
    "\n",
    "print(\"==== Simple Average Ensemble ====\")\n",
    "evaluate(y_true, y_pred_avg, prob_list_avg)\n",
    "print(\"\\n==== BIC-weighted Ensemble (approx BMA) ====\")\n",
    "evaluate(y_true, y_pred_bma, prob_list_bma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e334ce8-2a51-426b-9fcb-3f43096f7e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
