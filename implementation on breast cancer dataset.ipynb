{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32e1ef7",
   "metadata": {},
   "source": [
    "# Bayesian Network Structure Learning  \n",
    "### Using UCI Breast Cancer Diagnostic Dataset (WDBC)\n",
    "\n",
    "In this notebook, I implemented **Bayesian Network structure and parameter learning completely from scratch**, without using pgmpy or sklearn BN modules.\n",
    "\n",
    "Our pipeline includes:\n",
    "\n",
    "- Data loading and preprocessing  \n",
    "- Discretization of continuous features  \n",
    "- Manual Chi-square conditional independence test  \n",
    "- PC-Skeleton structure discovery  \n",
    "- Hill-Climbing score-based structure learning  \n",
    "- Manual Maximum Likelihood CPT estimation  \n",
    "- Simple BN inference for classification  \n",
    "- Model evaluation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b7b82fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 33)\n",
      "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
      "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")   # your file name\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4e37f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ID\n",
    "df = df.drop(columns=[\"id\"])\n",
    "\n",
    "# Convert diagnosis (M=malignant, B=benign)\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({\"B\": \"benign\", \"M\": \"malignant\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aec5320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis                    0\n",
       "radius_mean                  0\n",
       "texture_mean                 0\n",
       "perimeter_mean               0\n",
       "area_mean                    0\n",
       "smoothness_mean              0\n",
       "compactness_mean             0\n",
       "concavity_mean               0\n",
       "concave points_mean          0\n",
       "symmetry_mean                0\n",
       "fractal_dimension_mean       0\n",
       "radius_se                    0\n",
       "texture_se                   0\n",
       "perimeter_se                 0\n",
       "area_se                      0\n",
       "smoothness_se                0\n",
       "compactness_se               0\n",
       "concavity_se                 0\n",
       "concave points_se            0\n",
       "symmetry_se                  0\n",
       "fractal_dimension_se         0\n",
       "radius_worst                 0\n",
       "texture_worst                0\n",
       "perimeter_worst              0\n",
       "area_worst                   0\n",
       "smoothness_worst             0\n",
       "compactness_worst            0\n",
       "concavity_worst              0\n",
       "concave points_worst         0\n",
       "symmetry_worst               0\n",
       "fractal_dimension_worst      0\n",
       "Unnamed: 32                569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d32b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 32\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57cc2af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum().sum())  # Should print 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cf6c3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 31)\n",
      "Index(['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
      "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0010607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:296: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "feature_cols = df.columns[1:]   # all columns except diagnosis\n",
    "\n",
    "disc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "df[feature_cols] = disc.fit_transform(df[feature_cols]).astype(int).astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62a79a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"diagnosis\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75bee8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [\n",
    "    \"diagnosis\",\n",
    "    \"radius_mean\",\n",
    "    \"texture_mean\",\n",
    "    \"perimeter_mean\",\n",
    "    \"area_mean\",\n",
    "    \"smoothness_mean\",\n",
    "    \"compactness_mean\",\n",
    "    \"concavity_mean\",\n",
    "    \"symmetry_mean\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f60ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[selected]\n",
    "test_df = test_df[selected]\n",
    "vars = selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47af2f5",
   "metadata": {},
   "source": [
    "### Conditional Independence Test (Chi-Square)\n",
    "This function checks whether two variables are statistically independent.  \n",
    "I used chi-square tests because they work well for discrete variables.  \n",
    "This test is the foundation of the PC algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3e81699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from itertools import combinations\n",
    "\n",
    "def chi2_test(a, b):\n",
    "    table = pd.crosstab(a, b)\n",
    "    if table.shape[0] < 2 or table.shape[1] < 2:\n",
    "        return 1.0  # not enough data\n",
    "    chi2, p, _, _ = chi2_contingency(table)\n",
    "    return p\n",
    "\n",
    "def ci_test(df, X, Y, cond_set=None, alpha=0.05):\n",
    "    if not cond_set:\n",
    "        return chi2_test(df[X], df[Y]) >= alpha\n",
    "    \n",
    "    grouped = df.groupby(cond_set)\n",
    "    p_values = []\n",
    "    for _, subset in grouped:\n",
    "        if len(subset) < 5:\n",
    "            continue\n",
    "        p = chi2_test(subset[X], subset[Y])\n",
    "        p_values.append(p)\n",
    "    \n",
    "    if len(p_values) == 0:\n",
    "        return False  # treat dependent\n",
    "    \n",
    "    return np.mean(p_values) >= alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56ccc7",
   "metadata": {},
   "source": [
    "### PC Algorithm (Skeleton Discovery)\n",
    "The PC algorithm finds which variables are connected by removing edges if variables are conditionally independent.  \n",
    "I tested independence with conditioning sets of small size to keep computation fast.  \n",
    "The output is an undirected graph structure representing likely causal connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65aedd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_skeleton(df, vars, alpha=0.05, max_cond=1):\n",
    "    graph = {v: set(vars) - {v} for v in vars}\n",
    "    sep_sets = {}\n",
    "\n",
    "    for l in range(0, max_cond+1):\n",
    "        for X in vars:\n",
    "            for Y in list(graph[X]):\n",
    "                if X == Y: continue\n",
    "                neighbors = list(graph[X] - {Y})\n",
    "\n",
    "                if len(neighbors) < l:\n",
    "                    continue\n",
    "\n",
    "                for cond in combinations(neighbors, l):\n",
    "                    if ci_test(df, X, Y, list(cond), alpha):\n",
    "                        graph[X].remove(Y)\n",
    "                        graph[Y].remove(X)\n",
    "                        sep_sets[(X,Y)] = cond\n",
    "                        sep_sets[(Y,X)] = cond\n",
    "                        break\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a9dbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c77293",
   "metadata": {},
   "source": [
    "### Cycle Detection for DAG Validity\n",
    "Bayesian Networks must not contain cycles, so we check if a graph contains any directed loops.  \n",
    "I performed a DFS-based cycle check after every structural change.  \n",
    "This ensures the learned network is a valid DAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05a0dd",
   "metadata": {},
   "source": [
    "### BIC Score Calculation\n",
    "The BIC score measures how well a given network structure fits the data while penalizing complexity.  \n",
    "Higher BIC scores indicate better models.  \n",
    "This function is used by hill-climbing to decide which edge operations improve the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5abe8",
   "metadata": {},
   "source": [
    "### Hill-Climbing Structure Learning\n",
    "Hill-climbing searches for the best DAG by trying to add, remove, or reverse edges.  \n",
    "At each step, we keep the change only if it improves the BIC score.  \n",
    "This algorithm helps us find a good network structure efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c94e5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_cycle(edges, vars):\n",
    "    parents = {v: [] for v in vars}\n",
    "    for u, v in edges:\n",
    "        parents[v].append(u)\n",
    "    \n",
    "    visited = {v:0 for v in vars}\n",
    "\n",
    "    def dfs(v):\n",
    "        if visited[v] == 1:\n",
    "            return True\n",
    "        visited[v] = 1\n",
    "        for p in parents[v]:\n",
    "            if dfs(p):\n",
    "                return True\n",
    "        visited[v] = 2\n",
    "        return False\n",
    "\n",
    "    return any(dfs(v) for v in vars)\n",
    "\n",
    "\n",
    "def bic_score(df, edges, vars):\n",
    "    N = len(df)\n",
    "    parents = {v: [] for v in vars}\n",
    "\n",
    "    for u, v in edges:\n",
    "        parents[v].append(u)\n",
    "\n",
    "    LL = 0\n",
    "    num_params = 0\n",
    "\n",
    "    for child in vars:\n",
    "        par = parents[child]\n",
    "        child_states = sorted(df[child].unique())\n",
    "\n",
    "        if not par:\n",
    "            counts = df[child].value_counts()\n",
    "            total = sum(counts.values)\n",
    "\n",
    "            for state in child_states:\n",
    "                c = counts.get(state, 0)\n",
    "                if c > 0:\n",
    "                    p = c / total\n",
    "                    LL += c * np.log(p)\n",
    "\n",
    "            num_params += (len(child_states) - 1)\n",
    "            continue\n",
    "\n",
    "        # with parents\n",
    "        grouped = df.groupby(par)\n",
    "\n",
    "        for _, subset in grouped:\n",
    "            total = len(subset)\n",
    "            if total == 0:\n",
    "                continue\n",
    "\n",
    "            counts = subset[child].value_counts()\n",
    "\n",
    "            for state in child_states:\n",
    "                c = counts.get(state, 0)\n",
    "                if c > 0:\n",
    "                    p = c / total\n",
    "                    LL += c * np.log(p)\n",
    "\n",
    "            num_params += (len(child_states) - 1)\n",
    "\n",
    "    return LL - 0.5 * num_params * np.log(N)\n",
    "\n",
    "\n",
    "\n",
    "def hill_climb(df, vars):\n",
    "    edges = set()\n",
    "    best = bic_score(df, edges, vars)\n",
    "    improved = True\n",
    "\n",
    "    while improved:\n",
    "        improved = False\n",
    "        best_move = None\n",
    "\n",
    "        for X in vars:\n",
    "            for Y in vars:\n",
    "                if X == Y: \n",
    "                    continue\n",
    "\n",
    "                # Try add edge\n",
    "                if (X,Y) not in edges:\n",
    "                    new_edges = edges | {(X,Y)}\n",
    "                    if not has_cycle(new_edges, vars):\n",
    "                        score = bic_score(df, new_edges, vars)\n",
    "                        if score > best:\n",
    "                            best = score\n",
    "                            best_move = (\"add\", (X,Y))\n",
    "\n",
    "                # Try remove\n",
    "                if (X,Y) in edges:\n",
    "                    new_edges = edges - {(X,Y)}\n",
    "                    score = bic_score(df, new_edges, vars)\n",
    "                    if score > best:\n",
    "                        best = score\n",
    "                        best_move = (\"remove\", (X,Y))\n",
    "\n",
    "        if best_move:\n",
    "            typ, e = best_move\n",
    "            if typ == \"add\":\n",
    "                edges.add(e)\n",
    "            else:\n",
    "                edges.remove(e)\n",
    "            improved = True\n",
    "\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562d0ba",
   "metadata": {},
   "source": [
    "### Parameter Learning (MLE for CPTs)\n",
    "After learning the structure, we calculate the conditional probability tables (CPTs) for each node.  \n",
    "I computed probabilities directly from frequency counts in the training data.  \n",
    "These CPTs are required for performing inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d21f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_cpts(df, edges, vars):\n",
    "    parents = {v: [] for v in vars}\n",
    "    for u,v in edges:\n",
    "        parents[v].append(u)\n",
    "\n",
    "    cpts = {}\n",
    "\n",
    "    for child in vars:\n",
    "        pars = parents[child]\n",
    "\n",
    "        if not pars:\n",
    "            counts = df[child].value_counts()\n",
    "            total = counts.sum()\n",
    "            probs = {val: counts[val]/total for val in counts.index}\n",
    "            cpts[child] = {\"parents\": [], \"table\": {(): probs}}\n",
    "            continue\n",
    "\n",
    "        cpt_table = {}\n",
    "        grouped = df.groupby(pars)\n",
    "\n",
    "        for par_vals, subset in grouped:\n",
    "            par_vals = par_vals if isinstance(par_vals, tuple) else (par_vals,)\n",
    "            counts = subset[child].value_counts()\n",
    "            total = counts.sum()\n",
    "            probs = {v: counts.get(v,0)/total for v in df[child].unique()}\n",
    "            cpt_table[par_vals] = probs\n",
    "\n",
    "        cpts[child] = {\"parents\": pars, \"table\": cpt_table}\n",
    "\n",
    "    return cpts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c858056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_row(cpts, row, target=\"Class\"):\n",
    "    info = cpts[target]\n",
    "    parents = info[\"parents\"]\n",
    "\n",
    "    if not parents:\n",
    "        probs = list(info[\"table\"].values())[0]\n",
    "        return probs\n",
    "\n",
    "    par_vals = tuple(row[p] for p in parents)\n",
    "    if par_vals not in info[\"table\"]:\n",
    "        return {c: 1/len(info[\"table\"][()])}\n",
    "\n",
    "    return info[\"table\"][par_vals]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d66bb1",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "I compared predicted classes with actual test labels to measure accuracy and other metrics.  \n",
    "Metrics like precision and recall show how well the BN distinguishes between benign and malignant cases.  \n",
    "This gives a clear understanding of the BNâ€™s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d9b3089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8859649122807017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.88      0.94      0.91        72\n",
      "   malignant       0.89      0.79      0.84        42\n",
      "\n",
      "    accuracy                           0.89       114\n",
      "   macro avg       0.89      0.87      0.87       114\n",
      "weighted avg       0.89      0.89      0.88       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "vars = list(train_df.columns)\n",
    "edges = hill_climb(train_df, vars)  \n",
    "cpts = learn_cpts(train_df, edges, vars)\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    probs = predict_row(cpts, row, target=\"diagnosis\")\n",
    "    y_true.append(row[\"diagnosis\"])\n",
    "    pred = max(probs, key=probs.get)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811c167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
